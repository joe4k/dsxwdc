{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!pip install --upgrade watson-developer-cloud"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import json\nimport time"
        }, 
        {
            "source": "## Load data from DB2 Warehouse on the Cloud to Spark Dataframe\nSpecify the credentials for your DB2 Warehouse on the cloud instance and read table data into Spark data frame. To do so:\n\n- Click the Data icon (top right)\n- Choose the Connections tab\n- Select \"Insert SparkSession DataFrame\"\n   - Select the correct schema\n   - Choose Table DSX_CLOUDANT_SINGER_TWEETS\n\nThis should copy required code into the active notebook cell for accessing your DB2 Warehouse on the Cloud instance and read the table DSX_CLOUDANT_SINGER_TWEETS into a Spark dataframe.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "source": "## Data Exploration and Curation\nRun some analysis and exploration of the data to verify it is as expected", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# copy data into brandTweetsDF dataframe for processing\nbrandTweetsDF = data_df_1"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Return top 2 rows of Spark DataFrame\nbrandTweetsDF.limit(2).toPandas()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Print the schema of the loaded data\nbrandTweetsDF.printSchema()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## Drop unneeded columns\nbrandTweetsDF = brandTweetsDF.drop('_ID','_REV')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import datetime\nfrom datetime import date\nfrom dateutil import parser\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import DateType\n\n\ndef getDay(date):\n    print('input date: ', date)\n    day = parser.parse(str(date))\n    day = day.date()\n    return day\n\n# Add a field for the day the tweet was created (ignoring hour/minute/second)\nudfGetDay = udf(getDay, DateType())\n\nbrandTweetsDF = brandTweetsDF.withColumn('DAY',udfGetDay('CREATED_AT'))\n\n# Verify added field is as expected\nbrandTweetsDF.select(\"DAY\").limit(5).toPandas()"
        }, 
        {
            "source": "## Extract a Random Sample of Records\nNext, we will extract a randome rample of records to run NLU enrichment on. This is needed to make sure we don't exceed our limit of free NLU calls per day.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## Take a sample of the data\n## Limit to 1000 records as Watson NLU allows 1000 free calls per day\nimport random\n\nnum_records = brandTweetsDF.count()\nsample_num_records = 500\nfraction = float(sample_num_records)/float(num_records)\n\nseed = random.randint(1, 100)\nprint('Number of records: ', num_records, ' Sample size: ', sample_num_records, ' Fraction: ', fraction, ' Seed: ', seed)\nbrandTweetsSampleDF = brandTweetsDF.sample(False, fraction, seed)\n\n\n## Alternative Stratified Sampling approach\n## Returns RDD with length of 2, first col is the key (day) and second col is the original row for the key\n## Take only the actual data (column 1)\n## If you'd like to use this approach, uncomment the following 4 lines\n\n#fractionList = brandTweetsDF.rdd.map(lambda x: x['DAY']).distinct().map(lambda x: (x,fraction)).collectAsMap()\n#keybyday = brandTweetsDF.rdd.keyBy(lambda x: x['DAY'])\n#brandTweetsDFrdd = keybyday.sampleByKey(False,fractionList).map(lambda x: x[1])\n#brandTweetsSampleDF = spark.createDataFrame(brandTweetsDFrdd,brandTweetsDF.schema)\n\n\nprint('Number of records to send to NLU:', brandTweetsSampleDF.count())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# plot number of tweets per day\nfrom pyspark.sql import functions as F\nbrandTweetsSampleDFperDay = brandTweetsSampleDF.groupBy('DAY')\\\n                              .agg(F.count('ID')\\\n                              .alias('NUM_TWEETS_PER_DAY'))\nbrandTweetsSampleDFperDay.show()"
        }, 
        {
            "source": "Run a clean text function on all records to remove unwanted characters.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Add a step to clean the text by removing certain characters such as \\n, \\r, &amp, ...\nfrom pyspark.sql.types import StringType\n\ndef cleanText(text):\n    print('input text: ', text)\n    #utf8text = normalize('NFKD', text).encode('ascii','ignore').decode('ascii')\n    utf8text = text\n    text1 = utf8text.replace('\\n',' ')\n    text1 = text1.replace('//','')\n    text1 = text1.replace('\\\\3','')\n    text1 = text1.replace('\\r','')\n    text2 = text1.replace('&amp;',' ')\n    text2 = text2.replace('&lt;',' ')\n    text2 = text2.strip()\n    text_clean = text2.replace('\"','')\n    return text_clean\n\nudfcleanText = udf(cleanText, StringType())\n\nbrandTweetsCleanDF = brandTweetsSampleDF.withColumn('textnew',udfcleanText('TEXT_CLEAN'))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "brandTweetsCleanDF.count()"
        }, 
        {
            "source": "## Specify NLU Credentials\nNext, you need to specify the credentials for your Watson Natural Language Understanding (NLU) service. If you don't have an NLU service, you can create one by following [these instructions](https://console.bluemix.net/docs/services/natural-language-understanding/getting-started.html#getting-started-tutorial) and obtaining the service credentials. You need to specify the URL, username, and password.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Specify NLU credentials\ncredentials_json= {\n    \"nlu_url\":\"https://gateway.watsonplatform.net/natural-language-understanding/api\",\n    \"nlu_username\": \"3139b139-1699-4be0-84a1-28b0b4927271\",\n\t\"nlu_password\": \"BqVSOErX0jdo\",\n\t\"nlu_version\": \"2017-02-27\"\n}"
        }, 
        {
            "source": "## NLU Enrichment using REST datasource extension\nIn this notebook, we leverage the REST data source extension for Apache Spark as explained in the [blog](https://medium.com/ibm-data-science-experience/using-spark-as-a-parallel-processing-framework-for-accessing-rest-based-data-services-cd4c98526784) and [github repository](https://github.com/sourav-mazumder/Data-Science-Extensions/tree/master/spark-datasource-rest). The REST datasource extension allows us to leverage the distribtued compute power of Spark in making REST API calls. As explained on the github repository under the \"Using Rest Data Source in IBM Data Science Experience (DSx)\" section, you need to have access to your Apache Spark as a Service and upload the jar for this REST data source extension to your Apache Spark instance.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Specify the NLU endpoint\n# Note that the nlu_uri should be the complete endpoint \n# nlu_uri = \"https://gateway.watsonplatform.net/natural-language-understanding/api/v1/analyze?version=2017-02-27\"\nnlu_uri = credentials_json['nlu_url'] + \"/v1/analyze?version=\" + credentials_json['nlu_version']\n\nnlu_username = credentials_json['nlu_username']\nnlu_password = credentials_json['nlu_password']                                     "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql.functions import lit\n\nbd = brandTweetsCleanDF.selectExpr(\"textnew as text\")\n# Add a column titles features and specify the features you'd like NLU enrichment for, in this case keywords and sentiment\nbd = bd.withColumn('features',lit('keywords,sentiment'))\nbd.head(2)"
        }, 
        {
            "source": "### REST Datasource \nFor further detaiils on these parms, please consult the [following github repository](https://github.com/sourav-mazumder/Data-Science-Extensions/tree/master/spark-datasource-rest).\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "bd.selectExpr(\"text\",\"features\").createOrReplaceTempView(\"bdtbl\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\nnluprms = {'url' : nlu_uri, 'input' : 'bdtbl', 'method' : 'GET', 'userId':nlu_username, 'userPassword':nlu_password, 'callStrictlyOnce': 'Y', 'partitions': '10', 'connectionTimeout':'2000', 'readTimeout':'10000'}"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "start_time = time.time()\nbrandtweetsNLUDF = spark.read.format(\"org.apache.dsext.spark.datasource.rest.RestDataSource\").options(**nluprms).load()\n#print(brandtweetsNLUDF)\nprint(\"total run time for NLU enrichment using REST data source: \", time.time() - start_time)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "brandtweetsNLUDF.printSchema()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "brandtweetsNLUDF.head(5)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "brandtweetsNLUDF.count()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "brandtweetsNLUDF.limit(5).toPandas()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}