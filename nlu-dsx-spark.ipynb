{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!pip install --upgrade watson-developer-cloud"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import json\nimport time"
        }, 
        {
            "source": "## Load data from DB2 Warehouse on the Cloud to Spark Dataframe\nSpecify the credentials for your DB2 Warehouse on the cloud instance and read table data into Spark data frame. To do so:\n\n- Click the Data icon (top right)\n- Choose the Connections tab\n- Select \"Insert SparkSession DataFrame\"\n   - Select the correct schema\n   - Choose Table DSX_CLOUDANT_SINGER_TWEETS\n\nThis should copy required code into the active notebook cell for accessing your DB2 Warehouse on the Cloud instance and read the table DSX_CLOUDANT_SINGER_TWEETS into a Spark dataframe.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "source": "## Data Exploration and Curation\nRun some analysis and exploration of the data to verify it is as expected", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# copy data into brandTweetsDF dataframe for processing\nbrandTweetsDF = data_df_1"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Return top 2 rows of Spark DataFrame\nbrandTweetsDF.limit(2).toPandas()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Print the schema of the loaded data\nbrandTweetsDF.printSchema()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## Drop unneeded columns\nbrandTweetsDF = brandTweetsDF.drop('_ID','_REV')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import datetime\nfrom datetime import date\nfrom dateutil import parser\n\ndef getDay(date):\n    print('input date: ', date)\n    day = parser.parse(str(date))\n    day = day.date()\n    return day\n\n# Add a field for the day the tweet was created (ignoring hour/minute/second)\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import DateType\n\nudfGetDay = udf(getDay, DateType())\n\nbrandTweetsDF = brandTweetsDF.withColumn('DAY',udfGetDay('CREATED_AT'))\n\n# Verify added field is as expected\nbrandTweetsDF.select(\"DAY\").limit(5).toPandas()"
        }, 
        {
            "source": "## Extract a Random Sample of Records\nNext, we will extract a randome rample of records to run NLU enrichment on. This is needed to make sure we don't exceed our limit of free NLU calls per day.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## Take a sample of the data\n## Limit to 1000 records as Watson NLU allows 1000 free calls per day\nimport random\n\nnum_records = brandTweetsDF.count()\nsample_num_records = 500\nfraction = float(sample_num_records)/float(num_records)\n\nseed = random.randint(1, 100)\nprint('Number of records: ', num_records, ' Sample size: ', sample_num_records, ' Fraction: ', fraction, ' Seed: ', seed)\nbrandTweetsSampleDF = brandTweetsDF.sample(False, fraction, seed)\n\n\n## Alternative Stratified Sampling approach\n## Returns RDD with length of 2, first col is the key (day) and second col is the original row for the key\n## Take only the actual data (column 1)\n## If you'd like to use this approach, uncomment the following 4 lines\n\n#fractionList = brandTweetsDF.rdd.map(lambda x: x['DAY']).distinct().map(lambda x: (x,fraction)).collectAsMap()\n#keybyday = brandTweetsDF.rdd.keyBy(lambda x: x['DAY'])\n#brandTweetsDFrdd = keybyday.sampleByKey(False,fractionList).map(lambda x: x[1])\n#brandTweetsSampleDF = spark.createDataFrame(brandTweetsDFrdd,brandTweetsDF.schema)\n\n\nprint('Number of records to send to NLU:', brandTweetsSampleDF.count())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# plot number of tweets per day\nfrom pyspark.sql import functions as F\nbrandTweetsSampleDFperDay = brandTweetsSampleDF.groupBy('DAY')\\\n                              .agg(F.count('ID')\\\n                              .alias('NUM_TWEETS_PER_DAY'))\nbrandTweetsSampleDFperDay.show()"
        }, 
        {
            "source": "## Map to a Pandas dataframe and enrich with Watson Natural Language Understanding (NLU)\nNote that in order to call a REST API such as NLU on the sampled records, need to map the Spark data frame to a Pandas data frame and then execute the NLU enrichment using the Pandas data frame. This effectively runs the enrichment code on the master Spark node only.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create a Pandas frame and augment with Sentiment analysis and Keywords using Watson NLU\nbrandTweetsSamplePandasDF = brandTweetsSampleDF.toPandas()"
        }, 
        {
            "source": "## Specify NLU Credentials\nNext, you need to specify the credentials for your Watson Natural Language Understanding (NLU) service. If you don't have an NLU service, you can create one by following [these instructions](https://console.bluemix.net/docs/services/natural-language-understanding/getting-started.html#getting-started-tutorial) and obtaining the service credentials. You need to specify the URL, username, and password.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Specify NLU credentials\ncredentials_json= {\n    \"nlu_url\":\"YOUR_WATSON_NLU_URL\",\n    \"nlu_username\": \"YOUR WATSON NLU USERNAME\",\n\t\"nlu_password\": \"YOUR WATSON NLU PASSWORD\",\n\t\"nlu_version\": \"2017-02-27\"\n}"
        }, 
        {
            "source": "## Watson NLU Enrichment Definition\nIn this cell, import the Watson Developer Cloud Python SDK, parse the NLU credentials, and define the function to enrich text with NLU.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import watson_developer_cloud\n#import watson_developer_cloud.natural_language_understanding.features.v1 as features\nfrom watson_developer_cloud import NaturalLanguageUnderstandingV1\nfrom watson_developer_cloud.natural_language_understanding_v1 import Features, SentimentOptions, KeywordsOptions\nfrom watson_developer_cloud import WatsonException\n\n## Define credentials for NLU service\nnlu_url = credentials_json['nlu_url']\nnlu_username=credentials_json['nlu_username']\nnlu_password=credentials_json['nlu_password']\nnlu_version=credentials_json['nlu_version']\nnlu = watson_developer_cloud.NaturalLanguageUnderstandingV1(version = nlu_version,\n                                                            username = nlu_username,\n                                                            password = nlu_password)\n\n## Send text to NLU and extract Sentiment and Keywords\n## Make sure text is utf-8 encoded\ndef enrichNLU(text):\n    utf8text = text.encode(\"utf-8\")\n    # In python3, need to decode to string\n    utf8text = utf8text.decode('utf-8')\n    \n    try:\n        result = nlu.analyze(text = utf8text, features = Features(sentiment=SentimentOptions(),keywords=KeywordsOptions()))\n        sentiment = result['sentiment']['document']['score']\n        sentiment_label = result['sentiment']['document']['label']\n        keywords = list(result['keywords'])  \n    except WatsonException:\n        result = None\n        sentiment = 0.0\n        sentiment_label = None\n        keywords = None\n    #print sentiment\n    return sentiment, sentiment_label, keywords\n    #return result"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql.types import StringType\nfrom pyspark.sql.types import FloatType\nfrom pyspark.sql.types import ArrayType\nfrom pyspark.sql.types import StructType\nfrom pyspark.sql.types import StructField\n\nschema = brandTweetsSampleDF.schema\nschema1 = StructType([\n    StructField(\"relevance\", FloatType(), True),\n    StructField(\"text\", StringType(), True),                            \n])\n\nkeywordschema = StructType.fromJson(schema1.jsonValue())\nadded_fields = [StructField(\"SENTIMENT\", FloatType(), True),StructField(\"SENTIMENT_LABEL\",StringType(),True),\\\n                StructField(\"KEYWORDS\",ArrayType(keywordschema),True)] \n\nnewfields = StructType(schema.fields + added_fields)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import time\nstart_time = time.time()\n## This calls the enrichNLU function which accesses the Watson NLU API\nbrandTweetsSamplePandasDF['SENTIMENT'],brandTweetsSamplePandasDF['SENTIMENT_LABEL'],\\\nbrandTweetsSamplePandasDF['KEYWORDS'] = zip(*brandTweetsSamplePandasDF['TEXT_CLEAN'].map(enrichNLU))\nprint(brandTweetsSamplePandasDF)\nprint('total run time: ', time.time() - start_time)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "brandTweetsSamplePandasDF.count"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "a = u'bats\\u00E0'\nprint(a)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#v=\"RT : Retweet to vote for our boy!\\n\\nMy  vote for  is \\u201cDespacito\\u201d by   &amp;\\u2026 \"\nv2=r\"RT : Retweet to vote for \\U0001f36 our boy!\\n\\nMy  vote for  is \\u201cDespacito\\u201d by   &amp;\\u2026 \"\n#v2 = \"\\u2026\"\n#v2=\"u'RT : \"Despacito dominates  &amp; @JustinBieber has week\\'s three top-selling songs  https://t.co/UeqNbm\\u2026'\"\nv2= r\"RT : Despacito dominates  &amp; @JustinBieber \\U0001f36f has week\\'s three top-selling u'\\u2026' songs  https://t.co/UeqNbm \\u2026 &lt; // yes\"\n#v2=u'\\U0001f36f'\n#v2=r'\\u2026'\nprint(type(v2))\nif isinstance(v2, str):\n    print(\"yes\")\nelif isinstance(v2, bytes):\n    print(\"no\")\nv1=cleanText(v2)\n\nprint('v1: ', v1)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "i=0\nfor row in brandTweetsSampleDF.rdd.collect():\n    text = row['TEXT_CLEAN']\n    print \"i: \", i\n    i = i + 1\n    print \"text: \", text\n    tclean = cleanText(text)\n    print \"clean text: \", tclean"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "brandTweetsCleanDF.head(10)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "brandTweetsCleanDF.count()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#brandTweetsSampleDF.selectExpr(\"TEXT_CLEAN as text\").createOrReplaceTempView(\"brandtweetstbl\")\n#bd = brandTweetsSampleDF.selectExpr(\"TEXT_CLEAN as text\")\nbd = brandTweetsCleanDF.selectExpr(\"textnew as text\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "bd.head(2)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql.functions import lit\n#bd = brandTweetsSampleDF.withColumn('features',lit('keywords,sentiment'))\nbd = bd.withColumn('features',lit('keywords,sentiment'))\nbd.head(2)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql.functions import lit\n#bd = brandTweetsSampleDF.withColumn('features',lit('keywords,sentiment'))\nbd = bd.withColumn('features',lit('{\"keywords\":{},\"sentiment\":{}}'))\nbd.head(2)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "bd.selectExpr(\"text\",\"features\").createOrReplaceTempView(\"bdtbl\")\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "nlu_uri = \"https://gateway.watsonplatform.net/natural-language-understanding/api/v1/analyze?version=2017-02-27\""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "nluprms = {'url' : nlu_uri, 'input' : 'bdtbl', 'method' : 'GET', 'userId':nlu_username, 'userPassword':nlu_password, 'callStrictlyOnce': 'Y', 'partitions': '10', 'connectionTimeout':'2000', 'readTimeout':'10000'}\n#nluprms = {'url' : nlu_uri, 'input' : 'bdtbl', 'method' : 'POST', 'userId':nlu_username, 'userPassword':nlu_password, 'callStrictlyOnce': 'Y', 'partitions': '10', 'connectionTimeout':'3000', 'readTimeout':'15000'}"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "start_time = time.time()\n#%%time\n#start_time = time.clock()\nbrandtweetsNLUDF = spark.read.format(\"org.apache.dsext.spark.datasource.rest.RestDataSource\").options(**nluprms).load()\n#print(\"NLU enrichment in a parallel manner using REST datasource externsion execution took \", time.clock() - start_time, \"seconds\")\nprint(brandtweetsNLUDF)\nprint(\"total run time for REST data source: \", time.time() - start_time)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "brandtweetsNLUDF.printSchema()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "brandtweetsNLUDF.head(5)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "brandtweetsNLUDF.count()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql.functions import col\nbrandtweetsNLUDF.where(col(\"_corrupt_record\").isNotNull()).count()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tmpbrandtweetsNLUDF = brandtweetsNLUDF.where(col(\"_corrupt_record\").isNotNull())\ntmpbrandtweetsNLUDF.head(2)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "brandtweetsNLUDF.where(col(\"_corrupt_record\").isNull()).count()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print nlu_uri"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "brandtweetsNLUDF.limit(5).toPandas()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "d1 = brandtweetsNLUDF['output']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "d1.limit(5).toPandas()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Spark execution model\n# https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/\n"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}