{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "<div><img src=\"https://www.ibm.com/blogs/bluemix/wp-content/uploads/2017/02/NLU.png\", width=270, height=270, align = 'right'> </div>\n<div>\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/51/IBM_logo.svg/640px-IBM_logo.svg.png\", width = 90, height = 90, align = 'right', style=\"margin:0px 25px\"></div>\n\n# Extract Insights from Social Media with Watson Developer Cloud and Data Science Experience\n\nThis notebook shows you how you can use the Watson APIs and Watson Studio to analyze and visualize data from social media to get customer insights. Brand managers can use this information to obtain enhanced insights about customer preferences enabling them to make more accurate marketing decisions. For a more detailed description of the problem solved, solution architecture, data collection and creating/accessing the services involved, please consult <a href=\"https://www.ibm.com/developerworks/library/cc-cognitive-watson-extract-insights-spark-dsx/index.html\" target=\"_blank\" rel=\"noopener no referrer\">this tutorial</a>.\n\nThis notebook runs on Python 3.5 with Spark 2.3\n___________", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Table of contents\n\n1.  [Load the required libraries](#loadlibraries)\n2.  [Load data from Db2 Warehouse on Cloud](#loaddata)\n3.  [Perform some exploratory data analysis](#exploredata)\n4.  [Take a data sample](#takesample)\n5.  [Read credentials for NLU, Personality Insights, and Twitter](#getcredentials)\n6.  [Enrich the data with Watson NLU](#enrichnlu)\n7.  [Visualize sentiment and keywords](#sentiment)\n8.  [Enrich data with Watson Personality Insights](#enrichpi)\n9.  [Spark machine learning for user segmentation](#sparkml)\n10. [Visualize user segmentation](#clusters)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "__________________\n\n<a id=\"loadlibraries\"></a>\n## Step 1: Load the required libraries\n\n- <a href=\"https://github.com/watson-developer-cloud/python-sdk\" target=\"_blank\" rel=\"noopener no referrer\">watson-developer-cloud</a> is the Python SDK for Watson Developer Cloud services \n\n- <a href=\"http://www.tweepy.org/\" target=\"_blank\" rel=\"noopener no referrer\">tweepy</a> is a Python library for accessing Twitter API\n\n- <a href=\"https://github.com/amueller/word_cloud/\" target=\"_blank\" rel=\"noopener no referrer\">wordcloud</a> is a Python library for generating Word Clouds \n\n- <a href=\"https://pypi.python.org/pypi/plotly\" target=\"_blank\" rel=\"noopener no referrer\">plotly</a> is a Python library for making plots and charts", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "!pip install --upgrade watson-developer-cloud\n!pip install tweepy==3.3.0 --upgrade --force-reinstall\n!pip install --upgrade plotly\n!pip install --upgrade --force-reinstall wordcloud"
        }, 
        {
            "source": "________________\n\n<a id=\"loaddata\"> </a>\n## Step 2: Load data from Db2 Warehouse on Cloud\n\nThe first step is to load the data. This notebook assumes you have tweets already in a Db2 Warehouse on Cloud database. For more details about how to collect relevant tweets into a Db2 Warehouse on Cloud database, check out <a href=\"https://github.com/joe4k/twitterstreams\" target=\"_blank\" rel=\"noopener no referrer\">this</a> github repository.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Provide the required credentials to access your data from your DB2 Warehouse on Cloud instance. \n```\ndb2_properties = {\n    'jdbcurl': '*****',\n    'user': '*****',\n    'password': '*****'\n}\n```\n\nYou can obtain these credentials by loggin into your IBM Cloud account, and navigating to your DB2 Warehouse instance and clicking on Service Credentials. If Service Credentials are not auto-generated for your instance, click **New Credential** button and then copy the jdbcurl, user, and password information into the cell below.\n\nIn addition to the credentials you need to access your DB2 Warehouse instance, you need to specify the table which holds the data you'd like to analyze.\n\n**db2_table = (Your DB2 Schema).(Your DB2 Table)** \n\nFor example, the twitter data I am working with is stored in the following table:\n\n**db2_table = 'DASH6296.DSX_CLOUDANT_SINGERS_TWEETS'**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Start a Spark session\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## Make sure to provide the correct credentials in this cell so the DB2 Warehosue data can be read into a Spark dataframe.\ndb2_properties = {\n    'jdbcurl': 'jdbc:db2://dashdb-entry-yp-dal09-09.services.dal.bluemix.net:50000/BLUDB',\n    'user': '****',\n    'password': '*****'\n}\n\ndb2_table = 'DASH6296.DSX_CLOUDANT_SINGERS_TWEETS'"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "brandTweetsDF = spark.read.jdbc(db2_properties['jdbcurl'], table=db2_table, properties=db2_properties)\nbrandTweetsDF.head()"
        }, 
        {
            "source": "___________\n\n<a id=\"exploredata\"> </a>\n## Step 3: Run some exploratory data analysis", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "# Return top 2 rows of Spark DataFrame\nbrandTweetsDF.limit(2).toPandas()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Print the schema of the loaded data\nbrandTweetsDF.printSchema()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## Drop unneeded columns\nbrandTweetsDF = brandTweetsDF.drop('_ID','_REV')"
        }, 
        {
            "source": "Extract day from the `CREATED_AT` field. This is useful to plot tweet trends over time.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import datetime\nfrom datetime import date\nfrom dateutil import parser\n\ndef getDay(date):\n    print('input date: ', date)\n    day = parser.parse(str(date))\n    day = day.date()\n    return day\n\n# Add a field for the day the tweet was created (ignoring hour/minute/second)\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import DateType\n\nudfGetDay = udf(getDay, DateType())\n\nbrandTweetsDF = brandTweetsDF.withColumn('DAY',udfGetDay('CREATED_AT'))\n\n# Verify added field is as expected\nbrandTweetsDF.select(\"DAY\").limit(5).toPandas()"
        }, 
        {
            "source": "____________\n\n<a id=\"takesample\"> </a>\n## Step 4: Take a data sample\nFor purposes of this tutorial, we will work with a small sample of the data. In practice, you want to use large data sets for our analysis to capture as many insights as we can. However, to illustrate the approach, we can work with a small data set.\n\nFurthermore, we want to restrict the number of API calls to the free plan of <a href=\"https://www.ibm.com/watson/developercloud/\" target=\"_blank\" rel=\"noopener no referrer\">Watson Developer Cloud</a> services so users can run through the notebook successfully even if they only have access to free tier of the services.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## Take a sample of the data\n## Limit to 1000 records as Watson NLU allows 1000 free calls per day\nimport random\n\nnum_records = brandTweetsDF.count()\nsample_num_records = 950\nfraction = float(sample_num_records)/float(num_records)\n\nseed = random.randint(1, 100)\nprint('Number of records: ', num_records, ' Sample size: ', sample_num_records, ' Fraction: ', fraction, ' Seed: ', seed)\nbrandTweetsSampleDF = brandTweetsDF.sample(False, fraction, seed)\n\nprint('Number of records to send to NLU:', brandTweetsSampleDF.count())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# print number of tweets per day\nfrom pyspark.sql import functions as F\nbrandTweetsSampleDFperDay = brandTweetsSampleDF.groupBy('DAY')\\\n                              .agg(F.count('ID')\\\n                              .alias('NUM_TWEETS_PER_DAY'))\nbrandTweetsSampleDFperDay.show()"
        }, 
        {
            "source": "____________\n\n<a id=\"getcredentials\"> </a>\n## Step 5: Read the credentials for NLU, Personality Insights, and Twitter\nIn this step, we definte the credentials for NLU, Personality Insights and Twitter to be able to leverage these services. \n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "You can get the credentials for NLU and Personality Insights by logging into your IBM Cloud account, navigating to your NLU and PI instances (from your dashboard), and then for each of those service instances, click Service Credentials. If Service Credentials don't exist, you can click the New Credential button to create credentials.\n\n```\nnlu_creds = {\n    'apikey': '****',\n    'url': '****'\n}\n```\n\n```\npi_creds = {\n    'apikey': '****',\n    'url': '****'\n}\n```\n\nFor Twitter credentials, check the instructions on [this page](https://github.com/joe4k/twitterstreams) under Twitter Credentials.\n\n```\ntwitter_creds = {\n    'twitter_consumer_key': '****',\n    'twitter_consumer_secret': '****',\n    'twitter_access_token': '****',\n    'twitter_access_token_secret': '****'\n}\n```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Specify NLU credentials\nnlu_creds = {\n    'apikey': '****',\n    'url': '****'\n}\n\n# Specify PI credentials\npi_creds = {\n    'apikey': '****',\n    'url': '****'\n}\n\n# Specify Twitter credentials\ntwitter_creds = {\n    'twitter_consumer_key': '****',\n    'twitter_consumer_secret': '****',\n    'twitter_access_token': '****',\n    'twitter_access_token_secret': '****'\n}"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "source": "__________\n\n<a id=\"enrichnlu\"> </a>\n## Step 6: Enrich the data with  <a href=\"https://www.ibm.com/watson/developercloud/natural-language-understanding.html\" target=\"_blank\" rel=\"noopener no referrer\">Watson Natural Language Understanding (NLU)</a>\n\nWatson NLU allows us to extract sentiment and keywords from text.  In our case, the text is user tweets.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import json\nimport watson_developer_cloud\nfrom watson_developer_cloud import NaturalLanguageUnderstandingV1\nfrom watson_developer_cloud.natural_language_understanding_v1 import Features, SentimentOptions, KeywordsOptions\n\n\n## Define credentials for NLU service\nnlu_apikey=nlu_creds['apikey']\nnlu_url=nlu_creds['url']\n\nnlu = NaturalLanguageUnderstandingV1(\n    version='2018-03-16',\n    iam_apikey=nlu_apikey,\n    url=nlu_url\n)\n\n## Send text to NLU and extract Sentiment and Keywords\n## Make sure text is utf-8 encoded\ndef enrichNLU(text):\n    result = None\n    sentiment = 0.0\n    sentiment_label = None\n    keywords = None\n    \n    try:\n        result = nlu.analyze(text = text, features = Features(sentiment=SentimentOptions(),keywords=KeywordsOptions())).get_result()\n        #print(json.dumps(result,indent = 2))\n        sentiment = result['sentiment']['document']['score']\n        sentiment_label = result['sentiment']['document']['label']\n        keywords = list(result['keywords'])\n#        print(keywords)\n        kwdlist = []\n        for kwd in keywords:\n#            print('keyword list is: ', kwd)\n#            print('type of keyword list: ', type(kwd))\n#            print('tx: ', kwd['text'])\n            kwd_dict = {}\n            kwd_dict['text'] = kwd['text']\n            kwd_dict['count'] = kwd['count']\n            kwd_dict['relevance'] = float(kwd['relevance'])\n#            print('keyword dict: ', kwd_dict)\n            kwdlist.append(kwd_dict)\n#            print('kwd text:', kwd['text'])\n#            print(kwdlist)\n    except:\n        sentiment = 0.0\n        sentiment_label = None\n        kwdlist = None\n    return sentiment, sentiment_label, kwdlist"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "text = 'This is a great day in North Carolina as we celebrate'\ns_score, s_label, kwds = enrichNLU(text)\nprint(s_score, s_label, kwds)"
        }, 
        {
            "source": "We'll convert our dataframe to a Pandas dataframe and enrich the tweets with the results of Watson NLU.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create a Pandas frame and augment with Sentiment analysis and Keywords using Watson NLU\nbrandTweetsSamplePandasDF = brandTweetsSampleDF.toPandas()"
        }, 
        {
            "source": "**The next cell sends the records to the Watson API, so be aware of the number of calls that are being made. If you are on the free plan which is limited to 1000 API calls per day, you can only run this cell once. After that the server responds with a notice indicating you have reached the maximum number of API calls for the day.**  This could take 60+ seconds so some patience is required.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## This calls the enrichNLU function which accesses the Watson NLU API\nbrandTweetsSamplePandasDF['SENTIMENT'],brandTweetsSamplePandasDF['SENTIMENT_LABEL'],\\\nbrandTweetsSamplePandasDF['KEYWORDS'] = zip(*brandTweetsSamplePandasDF['TEXT_CLEAN'].map(enrichNLU))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# view top two records to verify Sentiment and Keywords enrichments are applied as expected\nprint('Rows x Columns for brandTweetsSampleDF:', brandTweetsSamplePandasDF.shape)\nbrandTweetsSamplePandasDF[:10] "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql.types import StringType\nfrom pyspark.sql.types import FloatType\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.types import ArrayType\nfrom pyspark.sql.types import StructType\nfrom pyspark.sql.types import StructField\n\nschema = brandTweetsSampleDF.schema\nschema1 = StructType([\n    StructField(\"text\", StringType(), True),                            \n    StructField(\"count\", IntegerType(), True),\n    StructField(\"relevance\", FloatType(), True)\n])\n\nkeywordschema = StructType.fromJson(schema1.jsonValue())\nadded_fields = [StructField(\"SENTIMENT\", FloatType(), True),StructField(\"SENTIMENT_LABEL\",StringType(),True),\\\n                StructField(\"KEYWORDS\",ArrayType(keywordschema),True)] \n\nnewfields = StructType(schema.fields + added_fields)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## Push the enriched data back out to Spark to continue working within the Spark API\nenrichedBrandTweetsDF = spark.createDataFrame(brandTweetsSamplePandasDF,newfields)\n\n# Print the schema of the Spark DataFrame to verify we have all the expected fields\nenrichedBrandTweetsDF.printSchema()"
        }, 
        {
            "source": "____________\n\n<a id=\"sentiment\"> </a>\n## Step 7: Visualize sentiment and keywords\n\nTwitter trends, sentiment, and keywords give a brand manager a view of a consumers' perceptions about the brand. These insights can be very useful to the brand manager. \n\nIn this step we visualize some of the data we received from NLU. Let's look at tweet trends and sentiment for **katyperry** and **taylorswift**. In addition, let's show the main keywords tweeted for both musicians.\n\nIn this section:<br/>\n7.1 [Compare sentiment between brand1, brand2 and brand3](#compare)<br/>\n7.2 [View sentiment over time for brands](#view)<br/>\n7.3 [Top keywords](#keywords)\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Separate tweets by brand\n# To do so, check if the tweet includes which brand and add a column to represent that\nbrandList=['katyperry','justinbieber','taylorswift']\ndef addBrand(text):\n    for brand in brandList:\n        if brand in text.lower():\n            return brand\n    return None"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\nudfAddBrand = udf(addBrand, StringType())\n\n# For purposes of separating tweets by brand, we need to run it against original TEXT and not the TEXT_CLEAN\n# This is because in several cases, the brand is referenced with a handle\nenrichedBrandsDF = enrichedBrandTweetsDF.withColumn('BRAND',udfAddBrand('TEXT'))\n\n# view top records to verify brand column extracted as expected\nenrichedBrandsDF.limit(2).toPandas()"
        }, 
        {
            "source": "\nTo visualize the difference in sentiment between brands, we'll create a separate dataframe for each brand.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql.functions import col\n\n## Create one DF for katyperry, one for taylorswift\nbrand1TweetsDF = enrichedBrandsDF.where(col('BRAND') == 'katyperry')\nbrand2TweetsDF = enrichedBrandsDF.where(col('BRAND') == 'justinbieber')\nbrand3TweetsDF = enrichedBrandsDF.where(col('BRAND') == 'taylorswift')\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Print out number of tweets for each brand in our sample\nprint(\"Number of tweets for katyperry: \", brand1TweetsDF.count())\nprint(\"Number of tweets for justinbieber: \", brand2TweetsDF.count())\nprint(\"Number of tweets for taylorswift: \", brand3TweetsDF.count())\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "brand1TweetsDF.head(2)"
        }, 
        {
            "source": "Now, find the number of tweets with different sentiment labels (Positive, Negative, Neutral) for **katyperry**, **justinbieber**, and **taylorswift**.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql import functions as F\nfrom pyspark.sql.functions import col\n\n## First for brand1\nbrand1TweetsDF = brand1TweetsDF.where(col('SENTIMENT_LABEL').isNotNull())\nbrand1SentimentDF = brand1TweetsDF.groupBy('SENTIMENT_LABEL')\\\n                              .agg(F.count('ID')\\\n                              .alias('NUM_TWEETS'))\n\n## Take a look\nbrand1SentimentDF.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## Now for brand2\nbrand2TweetsDF = brand2TweetsDF.where(col('SENTIMENT_LABEL').isNotNull())\nbrand2SentimentDF = brand2TweetsDF.groupBy('SENTIMENT_LABEL')\\\n                                .agg(F.count('ID')\\\n                                .alias('NUM_TWEETS'))\n\nbrand2SentimentDF.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## Now for brand3\nbrand3TweetsDF = brand3TweetsDF.where(col('SENTIMENT_LABEL').isNotNull())\nbrand3SentimentDF = brand3TweetsDF.groupBy('SENTIMENT_LABEL')\\\n                                .agg(F.count('ID')\\\n                                .alias('NUM_TWEETS'))\n\nbrand3SentimentDF.show()"
        }, 
        {
            "source": "### 7.1 Compare sentiment between brand1, brand2 and brand3<a id=\"compare\"> </a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## Pull aggregated back to driver and convert to Pandas dataframe for plotting\nbrand1SentimentDF = brand1SentimentDF.toPandas()\nbrand2SentimentDF = brand2SentimentDF.toPandas()\nbrand3SentimentDF = brand3SentimentDF.toPandas()\n\nimport matplotlib.pyplot as plt\n\n# Plot sentiment\n%matplotlib inline\nplot1_labels = brand1SentimentDF['SENTIMENT_LABEL']\nplot1_values = brand1SentimentDF['NUM_TWEETS']\nplot1_colors = ['green', 'gray', 'red']\n\nplot2_labels = brand2SentimentDF['SENTIMENT_LABEL']\nplot2_values = brand2SentimentDF['NUM_TWEETS']\nplot2_colors = ['green', 'gray', 'red']\n\nplot3_labels = brand3SentimentDF['SENTIMENT_LABEL']\nplot3_values = brand3SentimentDF['NUM_TWEETS']\nplot3_colors = ['green', 'gray', 'red']\n\nfig, axes = plt.subplots(nrows = 1, ncols = 3, figsize = (23, 10))\naxes[0].pie(plot1_values,  labels = plot1_labels, colors = plot1_colors, autopct = '%1.1f%%')\naxes[0].set_title('Percentage of Sentiment Values in all katyperry Tweets')\naxes[0].set_aspect('equal')\naxes[0].legend(loc = \"upper right\", labels=plot1_labels)\n\naxes[1].pie(plot2_values,  labels = plot2_labels, colors = plot2_colors, autopct = '%1.1f%%')\naxes[1].set_title('Percentage of Sentiment Values in all justinbieber Tweets')\naxes[1].set_aspect('equal')\naxes[1].legend(loc = \"upper right\", labels = plot2_labels)\n\naxes[2].pie(plot3_values,  labels = plot3_labels, colors = plot3_colors, autopct = '%1.1f%%')\naxes[2].set_title('Percentage of Sentiment Values in all taylorswift Tweets')\naxes[2].set_aspect('equal')\naxes[2].legend(loc = \"upper right\", labels = plot3_labels)\n\nfig.subplots_adjust(hspace = 1)\nplt.show()"
        }, 
        {
            "source": "### 7.2 View sentiment over time for brands<a id=\"view\"> </a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## Get aggregated sentiment for each day\nbrand1OverTimeDF = brand1TweetsDF\\\n                    .groupBy('DAY', 'SENTIMENT_LABEL')\\\n                    .agg(F.count('TEXT_CLEAN').alias('NUM_TWEETS'))\\\n                    .orderBy('DAY', ascending = True)\n\n## Get total tweets each day\nbrand1TweetsPerDayDF = brand1OverTimeDF.groupBy('DAY')\\\n                        .agg(F.sum('NUM_TWEETS').alias('NUM_TWEETS'))\\\n                        .orderBy('DAY', ascending = True)\n        \n## Convert back to Pandas\nbrand1OverTimeDF = brand1OverTimeDF.toPandas()\nbrand1TweetsPerDayDF = brand1TweetsPerDayDF.toPandas()\n\n## Identify rows with positive sentiment for each day\npositive1Index = brand1OverTimeDF['SENTIMENT_LABEL'] == 'positive'\npositive1TweetsDF = brand1OverTimeDF[positive1Index]\n\n## Identify rows with negative sentiment for each day\nnegative1Index = brand1OverTimeDF['SENTIMENT_LABEL'] == 'negative'\nnegative1TweetsDF = brand1OverTimeDF[negative1Index]\n\n## Check results\npositive1TweetsDF[:2]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## Repeat for brand2\n## Get aggregated sentiment for each day\nbrand2OverTimeDF = brand2TweetsDF\\\n                    .groupBy('DAY', 'SENTIMENT_LABEL')\\\n                    .agg(F.count('TEXT_CLEAN').alias('NUM_TWEETS'))\\\n                    .orderBy('DAY', ascending = True)\n\n## Get total tweets each day\nbrand2TweetsPerDayDF = brand2OverTimeDF.groupBy('DAY')\\\n                        .agg(F.sum('NUM_TWEETS').alias('NUM_TWEETS'))\\\n                        .orderBy('DAY', ascending = True)\n        \n## Convert back to Pandas\nbrand2OverTimeDF = brand2OverTimeDF.toPandas()\nbrand2TweetsPerDayDF = brand2TweetsPerDayDF.toPandas()\n\n## Identify rows with positive sentiment for each day\npositive2Index = brand2OverTimeDF['SENTIMENT_LABEL'] == 'positive'\npositive2TweetsDF = brand2OverTimeDF[positive2Index]\n\n## Identify rows with negative sentiment for each day\nnegative2Index = brand2OverTimeDF['SENTIMENT_LABEL'] == 'negative'\nnegative2TweetsDF = brand2OverTimeDF[negative2Index]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## Repeat for brand3\n## Get aggregated sentiment for each day\nbrand3OverTimeDF = brand3TweetsDF\\\n                    .groupBy('DAY', 'SENTIMENT_LABEL')\\\n                    .agg(F.count('TEXT_CLEAN').alias('NUM_TWEETS'))\\\n                    .orderBy('DAY', ascending = True)\n\n## Get total tweets each day\nbrand3TweetsPerDayDF = brand3OverTimeDF.groupBy('DAY')\\\n                        .agg(F.sum('NUM_TWEETS').alias('NUM_TWEETS'))\\\n                        .orderBy('DAY', ascending = True)\n        \n## Convert back to Pandas\nbrand3OverTimeDF = brand3OverTimeDF.toPandas()\nbrand3TweetsPerDayDF = brand3TweetsPerDayDF.toPandas()\n\n## Identify rows with positive sentiment for each day\npositive3Index = brand3OverTimeDF['SENTIMENT_LABEL'] == 'positive'\npositive3TweetsDF = brand3OverTimeDF[positive3Index]\n\n## Identify rows with negative sentiment for each day\nnegative3Index = brand3OverTimeDF['SENTIMENT_LABEL'] == 'negative'\nnegative3TweetsDF = brand3OverTimeDF[negative3Index]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "## Define values for matplotlib\nx = brand1TweetsPerDayDF['DAY']\ny1 = brand1TweetsPerDayDF['NUM_TWEETS']\npy1 = positive1TweetsDF['NUM_TWEETS']\nny1 = negative1TweetsDF['NUM_TWEETS']\n\ny2 = brand2TweetsPerDayDF['NUM_TWEETS']\npy2 = positive2TweetsDF['NUM_TWEETS']\nny2 = negative2TweetsDF['NUM_TWEETS']\n\ny3 = brand3TweetsPerDayDF['NUM_TWEETS']\npy3 = positive3TweetsDF['NUM_TWEETS']\nny3 = negative3TweetsDF['NUM_TWEETS']\n\nfig, axes = plt.subplots(nrows=4, ncols=1, figsize=(20, 10))\naxes[0].plot(range(len(y1)), y1, linewidth=2, color='purple')\naxes[0].plot(range(len(y2)), y2, linewidth=2, color='blue')\naxes[0].plot(range(len(y3)), y3, linewidth=2, color='yellow')\naxes[0].set_xticks(x.index.tolist())\naxes[0].set_xticklabels([date.strftime(\"%Y-%m-%d\") for date in x])\naxes[0].margins = 0\naxes[0].set_xlabel('Date/Time')\naxes[0].set_ylabel('Num of Tweets')\naxes[0].set_title('Number of Tweets Over Time for All Musicians')\naxes[0].set_xlim(0, len(y1))\naxes[0].legend(loc=\"upper right\", labels=['katyperry','justinbieber','taylorswift'])\n\naxes[1].plot(range(len(y1)), y1, linewidth=2, color='blue')\naxes[1].plot(range(len(py1)), py1, linewidth=2, color='green')\naxes[1].plot(range(len(ny1)), ny1, linewidth=2, color='red')\naxes[1].set_xticks(x.index.tolist())\naxes[1].set_xticklabels([date for date in x])\naxes[1].margins = 0\naxes[1].set_xlabel('Date/Time')\naxes[1].set_ylabel('Num of Tweets')\naxes[1].set_title('Number of Tweets Over Time for katyperry - All, Positive and Negative')\naxes[1].set_xlim(0, len(y1))\naxes[1].legend(loc=\"upper right\", labels=['All Tweets', 'Positive', 'Negative'])\n\naxes[2].plot(range(len(y2)), y2, linewidth=2, color='blue')\naxes[2].plot(range(len(py2)), py2, linewidth=2, color='green')\naxes[2].plot(range(len(ny2)), ny2, linewidth=2, color='red')\naxes[2].set_xticks(x.index.tolist())\naxes[2].set_xticklabels([date for date in x])\naxes[2].margins = 0\naxes[2].set_xlabel('Date/Time')\naxes[2].set_ylabel('Num of Tweets')\naxes[2].set_title('Number of Tweets Over Time for justinbieber - All, Positive and Negative')\naxes[2].set_xlim(0, len(y1))\naxes[2].legend(loc=\"upper right\", labels=['All Tweets', 'Positive', 'Negative'])\n\naxes[3].plot(range(len(y3)), y3, linewidth=2, color='blue')\naxes[3].plot(range(len(py3)), py3, linewidth=2, color='green')\naxes[3].plot(range(len(ny3)), ny3, linewidth=2, color='red')\naxes[3].set_xticks(x.index.tolist())\naxes[3].set_xticklabels([date for date in x])\naxes[3].margins = 0\naxes[3].set_xlabel('Date/Time')\naxes[3].set_ylabel('Num of Tweets')\naxes[3].set_title('Number of Tweets Over Time for taylorswift - All, Positive and Negative')\naxes[3].set_xlim(0, len(y1))\naxes[3].legend(loc=\"upper right\", labels=['All Tweets', 'Positive', 'Negative'])\n\n## Rotate x-axes for legibility.\nfor ax in fig.axes:\n    plt.sca(ax)\n    plt.xticks(rotation = 45)\n\nfig.subplots_adjust(hspace=1)\nplt.show()"
        }, 
        {
            "source": "### 7.3 Top keywords<a id=\"keywords\"> </a>\nExtract and display top keywords expressed in the tweets referring to the brands.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql.functions import explode\n\n# Explode keywords\nbrand1KeywordsDF = brand1TweetsDF.select(explode('KEYWORDS').alias('TOPKEYWORDS'))\nbrand2KeywordsDF = brand2TweetsDF.select(explode('KEYWORDS').alias('TOPKEYWORDS'))\nbrand3KeywordsDF = brand3TweetsDF.select(explode('KEYWORDS').alias('TOPKEYWORDS'))\n\nbrand1TopKeywordsDF = brand1KeywordsDF.select('TOPKEYWORDS').rdd.map(lambda row: row[0]).toDF()\nbrand2TopKeywordsDF = brand2KeywordsDF.select('TOPKEYWORDS').rdd.map(lambda row: row[0]).toDF()\nbrand3TopKeywordsDF = brand3KeywordsDF.select('TOPKEYWORDS').rdd.map(lambda row: row[0]).toDF()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# UDF to filter non-insightful words such as profanity words or stop words\n# Add all stop words and non-insightful words to this list\nfilterList = ['today', 'now', 'to', 'yes']\ndef filter_profanity(word):\n    if word in filterList:\n        return None\n    if \"http\" in word:\n        return None\n    return word\n\n# UDF to return lower case of word\ndef toLowerCase(word):\n    return word.lower()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Process extracted keywords to filter profanity and change to lower case\nudfLowerCase = udf(toLowerCase, StringType())\nbrand1TopKeywordsDF = brand1TopKeywordsDF.withColumn('TOPKEYWORDS',udfLowerCase('text'))\nbrand2TopKeywordsDF = brand2TopKeywordsDF.withColumn('TOPKEYWORDS',udfLowerCase('text'))\nbrand3TopKeywordsDF = brand3TopKeywordsDF.withColumn('TOPKEYWORDS',udfLowerCase('text'))\n\nudfFilterProfanity = udf(filter_profanity, StringType())\nbrand1TopKeywordsDF = brand1TopKeywordsDF.withColumn('TOPKEYWORDS',udfFilterProfanity('TOPKEYWORDS'))\nbrand2TopKeywordsDF = brand2TopKeywordsDF.withColumn('TOPKEYWORDS',udfFilterProfanity('TOPKEYWORDS'))\nbrand3TopKeywordsDF = brand3TopKeywordsDF.withColumn('TOPKEYWORDS',udfFilterProfanity('TOPKEYWORDS'))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Group by TOPKEYWORDS and computer average relevance per keyword and also number of tweets for each keyword\nbrand1KwdsNumDF = brand1TopKeywordsDF.groupBy('TOPKEYWORDS').agg(F.count('TOPKEYWORDS').alias('KWDSNUMTWEETS'))\nbrand2KwdsNumDF = brand2TopKeywordsDF.groupBy('TOPKEYWORDS').agg(F.count('TOPKEYWORDS').alias('KWDSNUMTWEETS'))\nbrand3KwdsNumDF = brand3TopKeywordsDF.groupBy('TOPKEYWORDS').agg(F.count('TOPKEYWORDS').alias('KWDSNUMTWEETS'))\n\nbrand1KwdsRelDF = brand1TopKeywordsDF.groupBy('TOPKEYWORDS').agg(F.avg('relevance').alias('KWDSAVGRELEVANCE'))\nbrand2KwdsRelDF = brand2TopKeywordsDF.groupBy('TOPKEYWORDS').agg(F.avg('relevance').alias('KWDSAVGRELEVANCE'))\nbrand3KwdsRelDF = brand3TopKeywordsDF.groupBy('TOPKEYWORDS').agg(F.avg('relevance').alias('KWDSAVGRELEVANCE'))\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# join dataframes into one\nbrand1TweetsKeywordsDF = brand1KwdsNumDF.join(brand1KwdsRelDF,'TOPKEYWORDS','outer')\nbrand2TweetsKeywordsDF = brand2KwdsNumDF.join(brand2KwdsRelDF,'TOPKEYWORDS','outer')\nbrand3TweetsKeywordsDF = brand3KwdsNumDF.join(brand3KwdsRelDF,'TOPKEYWORDS','outer')\n\n# Define keyword score as product of number of tweets expressing that keyword and average relevance\nbrand1TweetsKeywordsDF = brand1TweetsKeywordsDF.withColumn('KEYWORD_SCORE',brand1TweetsKeywordsDF.KWDSNUMTWEETS * brand1TweetsKeywordsDF.KWDSAVGRELEVANCE)\nbrand2TweetsKeywordsDF = brand2TweetsKeywordsDF.withColumn('KEYWORD_SCORE',brand2TweetsKeywordsDF.KWDSNUMTWEETS * brand2TweetsKeywordsDF.KWDSAVGRELEVANCE)\nbrand3TweetsKeywordsDF = brand3TweetsKeywordsDF.withColumn('KEYWORD_SCORE',brand3TweetsKeywordsDF.KWDSNUMTWEETS * brand3TweetsKeywordsDF.KWDSAVGRELEVANCE)\n\n# Sort dataframe in descending order of KEYWORD_SCORE\nbrand1TweetsKeywordsDF = brand1TweetsKeywordsDF.orderBy('KEYWORD_SCORE',ascending=False)\nbrand2TweetsKeywordsDF = brand2TweetsKeywordsDF.orderBy('KEYWORD_SCORE',ascending=False)\nbrand3TweetsKeywordsDF = brand3TweetsKeywordsDF.orderBy('KEYWORD_SCORE',ascending=False)\n\n# Remove None keywords\nbrand1TweetsKeywordsDF = brand1TweetsKeywordsDF.where(col('TOPKEYWORDS').isNotNull())\nbrand2TweetsKeywordsDF = brand2TweetsKeywordsDF.where(col('TOPKEYWORDS').isNotNull())\nbrand3TweetsKeywordsDF = brand3TweetsKeywordsDF.where(col('TOPKEYWORDS').isNotNull())\n\n# Remove the brand name from the list of top keywords\n# Note we want to keey one brand name in another brand's list because that could be of interest\nbrand1TweetsKeywordsDF = brand1TweetsKeywordsDF.where(col('TOPKEYWORDS') != \"katy\")\nbrand1TweetsKeywordsDF = brand1TweetsKeywordsDF.where(col('TOPKEYWORDS') != \"katy perry\")\nbrand1TweetsKeywordsDF = brand1TweetsKeywordsDF.where(col('TOPKEYWORDS') != \"katyperry\")\n\nbrand2TweetsKeywordsDF = brand2TweetsKeywordsDF.where(col('TOPKEYWORDS') != \"justin\")\nbrand2TweetsKeywordsDF = brand2TweetsKeywordsDF.where(col('TOPKEYWORDS') != \"justin bieber\")\nbrand2TweetsKeywordsDF = brand2TweetsKeywordsDF.where(col('TOPKEYWORDS') != \"justinbieber\")\n\nbrand3TweetsKeywordsDF = brand3TweetsKeywordsDF.where(col('TOPKEYWORDS') != \"taylor\")\nbrand3TweetsKeywordsDF = brand3TweetsKeywordsDF.where(col('TOPKEYWORDS') != \"taylor swift\")\nbrand3TweetsKeywordsDF = brand3TweetsKeywordsDF.where(col('TOPKEYWORDS') != \"taylorswift\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(\"Top Keywords from tweets mentioning katyperry\")\nbrand1TweetsKeywordsDF.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(\"Top Keywords from tweets mentioning justinbieber\")\nbrand2TweetsKeywordsDF.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(\"Top Keywords from tweets mentioning taylorsiwft\")\nbrand3TweetsKeywordsDF.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "brand1TweetsKeywordsPandas = brand1TweetsKeywordsDF.toPandas()\nbrand2TweetsKeywordsPandas = brand2TweetsKeywordsDF.toPandas()\nbrand3TweetsKeywordsPandas = brand3TweetsKeywordsDF.toPandas()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!pip install --upgrade --force-reinstall wordcloud"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from wordcloud import WordCloud\n\n# Process Pandas DataFrame in the right format to leverage wordcloud.py for plotting\n# See documentation: https://github.com/amueller/word_cloud/blob/master/wordcloud/wordcloud.py \ndef prepForWordCloud(pandasDF,n):\n    kwdList = pandasDF['TOPKEYWORDS']\n    sizeList = pandasDF['KEYWORD_SCORE']\n    kwdSize = {}\n    for i in range(n):\n        kwd=kwdList[i]\n        size=sizeList[i]\n        kwdSize[kwd] = size\n    return kwdSize"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "%matplotlib inline\nmaxWords = len(brand1TweetsKeywordsPandas)\nnWords = 20\n\n#Generating wordcloud. Relative scaling value is to adjust the importance of a frequency word.\n#See documentation: https://github.com/amueller/word_cloud/blob/master/wordcloud/wordcloud.py\nbrand1KwdFreq = prepForWordCloud(brand1TweetsKeywordsPandas,nWords)\nbrand1WordCloud = WordCloud(max_words=maxWords,relative_scaling=0,normalize_plurals=False).generate_from_frequencies(brand1KwdFreq)\n\nbrand2KwdFreq = prepForWordCloud(brand2TweetsKeywordsPandas,nWords)\nbrand2WordCloud = WordCloud(max_words=nWords,relative_scaling=0,normalize_plurals=False).generate_from_frequencies(brand2KwdFreq)\n\nbrand3KwdFreq = prepForWordCloud(brand3TweetsKeywordsPandas,nWords)\nbrand3WordCloud = WordCloud(max_words=nWords,relative_scaling=0,normalize_plurals=False).generate_from_frequencies(brand3KwdFreq)\n\n\nfig, ax = plt.subplots(nrows = 1, ncols = 3, figsize = (23, 10))\n\n# Set titles for images\nax[0].set_title('Top Keywords for katyperry')\nax[1].set_title('Top Keywords for justinbieber')\nax[2].set_title('Top Keywords for taylorswift')\n\n                \n# Plot word clouds\nax[0].imshow(brand1WordCloud)\nax[1].imshow(brand2WordCloud)\nax[2].imshow(brand3WordCloud)\n\n# turn off axis and ticks\nplt.axis(\"off\")\nax[0].tick_params(axis='both',which='both',bottom='off',top='off',left='off',right='off',\n                 labelbottom='off',labeltop='off',labelleft='off',labelright='off') \n\nax[1].tick_params(axis='both',which='both',bottom='off',top='off',left='off',right='off',\n                 labelbottom='off',labeltop='off',labelleft='off',labelright='off') \n\nax[2].tick_params(axis='both',which='both',bottom='off',top='off',left='off',right='off',\n                 labelbottom='off',labeltop='off',labelleft='off',labelright='off') \n\nplt.show()"
        }, 
        {
            "source": "_____________\n\n<a id=\"enrichpi\"> </a>\n## Step 8: Enrich the data with <a href=\"https://www.ibm.com/watson/developercloud/personality-insights.html\" target=\"_blank\" rel=\"noopener no referrer\">Watson Personality Insights (PI)</a>\n\nIn this tutorial, we will create personality profiles for a sample of 100 users as that is the limit of what we can run with free plan for Watson <a href=\"https://www.ibm.com/watson/developercloud/personality-insights.html\" target=\"_blank\" rel=\"noopener no referrer\">Personality Insights (PI)</a>. \n\nIn practice, you can run personality profiles for all users, or you can choose to run personality profiles for only a selected subset of users; for example, for users with most negative sentiment tweets or users with largest number of followers or posts.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import random\n## Aggregate users by sentiment\nbrandUserSentimentDF = brand2TweetsDF\\\n                        .groupBy('USER_SCREEN_NAME', 'SENTIMENT_LABEL','SENTIMENT',\\\n                                 'USER_FAVOURITES_COUNT','USER_STATUSES_COUNT',\n                                 'USER_FOLLOWERS_COUNT','USER_FRIENDS_COUNT')\\\n                        .count()\\\n                        .orderBy('SENTIMENT', ascending = True)\n\n## Get negative and positive tweeting users\nnegativeTweetersDF = brandUserSentimentDF.where(col('SENTIMENT_LABEL') == \"negative\")\npositiveTweetersDF = brandUserSentimentDF.where(col('SENTIMENT_LABEL') == \"positive\")  \n\n# Take a random sample of 100 users\nnum_users = brandUserSentimentDF.count()\nsample_num_users = 95\nusrfraction = float(sample_num_users)/float(num_users)\nusrseed = random.randint(1, 100)\n\n## Start off by finding the number of unique users tweeting about Coke\nprint('Number of unique users tweeting about justinbieber: ', len(brand2TweetsDF.select('USER_SCREEN_NAME').distinct().collect()))\nprint('Number of negative tweeting folks: ', negativeTweetersDF.count())\nprint('Number of positive tweeting folks: ', positiveTweetersDF.count())\nprint('Sample size: ', sample_num_users, ' Fraction: ', usrfraction, ' Seed: ', usrseed)\n\nbrandUserSampleDF = brandUserSentimentDF.sample(False, usrfraction, usrseed)\n\nprint('Records in brandUserSampleDF: ', brandUserSampleDF.count())"
        }, 
        {
            "source": "In this step, we collect enough tweets for each unique user and run those tweets through Personality Insights to extract the Big Five personality traits, also known as OCEAN (openness, conscientiousness, extraversion, agreeableness, and neuroticism).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## Libraries and Credentials for Twitter and Personality Insights\n#import urllib2, requests, json\nimport tweepy\nfrom tweepy import OAuthHandler\nfrom watson_developer_cloud import PersonalityInsightsV3\n\n## Credentials for Twitter Developer account\nconsumer_key = twitter_creds['twitter_consumer_key']\nconsumer_secret = twitter_creds['twitter_consumer_secret']\naccess_token = twitter_creds['twitter_access_token']\naccess_token_secret = twitter_creds['twitter_access_token_secret'] \n\nauth = OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\n \napi = tweepy.API(auth)\n\n## Get PI object using PI credentials\npersonality_insights = PersonalityInsightsV3(\n    version='2017-10-13',\n    iam_apikey=pi_creds['apikey'],\n    url=pi_creds['url']\n)"
        }, 
        {
            "source": "Now we'll define several functions to programmatically paint a portrait of each user and their personality.\n\n- `getTweets()` - to collect tweets from a given user ID\n- `getPersonality()` - to call Personality Insights on the users' tweets\n- `extractOCEANtraits()` - to gather the OCEAN scores for each users PI data\n- `getPItraits()` - to put it all together and return a list of traits for each user", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## Collect tweets for a given user\ndef getTweets(username):\n    twitter_id = username    \n    try:\n        tweet_collection = api.user_timeline(screen_name = twitter_id, count = 100, include_rts = True)\n        i = 0\n        tweets = []\n        for status in tweet_collection:\n            i = i+1\n            tweets.append(status.text)\n    except Exception:\n        tweets = None\n    return tweets\n\n## Call Personality Insights on the tweets for the user\ndef getPersonality(tweets):\n    # get tweets by user\n    if tweets == None:\n        profile = None\n    else:\n        tweets_content = ' '.join(tweets)\n        # UTF-8 encoding\n        twt = tweets_content.encode('utf-8')\n        # call PI to get personality profile\n        try:\n            profile = personality_insights.profile(twt, content_type = 'text/plain;charset=utf-8', content_language = None,\n                                                   accept= 'application/json', accept_language = None,\n                                                   raw_scores = False, consumption_preferences = False,\n                                                   csv_headers = False).get_result()\n\n        except Exception:\n            profile = None\n    \n    return profile\n\n\n## Extract OCEAN percentiles from PI data\ndef extractOCEANtraits(profile):\n    if profile == None:\n        openness = None\n        conscientiousness = None\n        extraversion = None\n        agreeableness = None\n        neuroticism = None\n    else:\n        personality = profile['personality']\n        openness = personality[0]['percentile']\n        conscientiousness = personality[1]['percentile']\n        extraversion = personality[2]['percentile']\n        agreeableness = personality[3]['percentile']\n        neuroticism = personality[4]['percentile']\n    \n    return openness, conscientiousness, extraversion, agreeableness, neuroticism\n\n## Combine function calls for a user\ndef getPItraits(user, verbose = F):\n    # get tweets by user\n    if verbose == F:\n        try:\n            tweets = getTweets(user)\n            # run PI profile on extracted tweets\n            profile = getPersonality(tweets)\n            # extract OCEAN traits\n            openness, conscientiousness, extraversion, agreeableness, neuroticism = extractOCEANtraits(profile)\n        except Exception:\n            return None\n        return openness, conscientiousness, extraversion, agreeableness, neuroticism\n    else:\n        print('Getting tweets for user: ', user)\n        try:\n            tweets = getTweets(user)\n            # run PI profile on extracted tweets\n            profile = getPersonality(tweets)\n            # extract OCEAN traits\n            openness, conscientiousness, extraversion, agreeableness, neuroticism = extractOCEANtraits(profile)\n        except Exception:\n            return None\n        return openness, conscientiousness, extraversion, agreeableness, neuroticism"
        }, 
        {
            "source": "Now we'll extract Personality Insights traits for the users sampled in step 7, then push the data back to Spark for machine learning.  \n\n**This cell will also send data to the API, so be mindful of the number of calls you are making and be patient for the results.**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## Capture schema\nschema = brandUserSampleDF.schema\n\nadded_fields = [StructField(\"OPENNESS\", FloatType(), True),\n                StructField(\"CONSCIENTIOUSNESS\", FloatType(), True),\n                StructField(\"EXTRAVERSION\", FloatType(), True),\n                StructField(\"AGREEABLENESS\", FloatType(), True),\n                StructField(\"NEUROTICISM\", FloatType(), True)\n                ] \n\nnewfields = StructType(schema.fields + added_fields)\n\n## Convert to Pandas to use the Watson PI API\nbrandUserSampleDF = brandUserSampleDF.toPandas()\nbrandUserSampleDF['OPENNESS'], brandUserSampleDF['CONSCIENTIOUSNESS'],\\\nbrandUserSampleDF['EXTRAVERSION'], brandUserSampleDF['AGREEABLENESS'],\\\nbrandUserSampleDF['NEUROTICISM'] = zip(*brandUserSampleDF['USER_SCREEN_NAME'].map(getPItraits))\n\n# Conver back to Spark Dataframe from Pandas dataframe\nuserPersonalityDF = spark.createDataFrame(brandUserSampleDF,newfields)\n\n## Drop rows without any PI enrichment\nuserPersonalityDF = userPersonalityDF.na.drop()\n\n## Check row count and schema\nprint('Number of records in userPersonalityInsightsDF: ', userPersonalityDF.count())\nuserPersonalityDF.printSchema()\n\nuserPersonalityDF.limit(2).toPandas()"
        }, 
        {
            "source": "________________\n\n<a id=\"sparkml\"> </a>\n## Step 9: Spark machine learning for user segmentation \n<a href=\"https://spark.apache.org/docs/latest/ml-guide.html\" target=\"_blank\" rel=\"noopener no referrer\">Spark MLlib</a>  includes a rich set of machine learning algorithms that are very powerful in extracting insights from data. Typically, you will need to convert the data into the right format before you can apply these machine learning algorithms. In this step, we apply several steps to process the data so it can run the <a href=\"https://spark.apache.org/docs/latest/mllib-clustering.html#k-means\" target=\"_blank\" rel=\"noopener no referrer\">Kmeans</a> clustering algorithm including normalizing **USER_FOLLOWERS_COUNT** and **USER_STATUSES_COUNT** fields as well as extracting the relevant feature set into a Vector to be used for clustering.\n\nWe run Kmeans clustering using two different feature sets.\n* **FeatureSet 1** (no Personality Traits): (SENTIMENT, USER_FOLLOWERS_COUNT, USER_STATUSES_COUNT)\n* **FeatureSet 2** (with Personality Traits): (SENTIMENT, USER_FOLLOWERS_COUNT, USER_STATUSES_COUNT, OPENNESS, CONSCIENTIOUSNESS, EXTRAVERSION, AGREEABLENESS, NEUROTICISM)\n\nFirst, we'll convert the follower and statuses counts to a vector as per the Spark documentation. Then we can run <a href=\"https://spark.apache.org/docs/2.1.0/ml-features.html#minmaxscaler\" target=\"_blank\" rel=\"noopener no referrer\">MinMaxScaler</a> and normalize the counts to a range between 0 and 1. These are necessary preprocessing steps for the clustering algorithm to work properly.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import MinMaxScaler\nfrom pyspark.ml.linalg import Vectors\n\n## Define columns to be converted to vectors\nfollowersVector = VectorAssembler(\n  inputCols=[\"USER_FOLLOWERS_COUNT\"], outputCol=\"USER_FOLLOWERS_COUNT_VECTOR\")\n\nstatusesVector = VectorAssembler(\n  inputCols=[\"USER_STATUSES_COUNT\"], outputCol=\"USER_STATUSES_COUNT_VECTOR\")\n\n## Define our input and output columns for MinMaxScaler\nfollowersScaler = MinMaxScaler(inputCol=\"USER_FOLLOWERS_COUNT_VECTOR\", outputCol=\"USER_FOLLOWERS_COUNT_SCALED\")\nstatusesScaler = MinMaxScaler(inputCol=\"USER_STATUSES_COUNT_VECTOR\", outputCol=\"USER_STATUSES_COUNT_SCALED\")\n\n## Invoke the VectorAssembler transformations and select desired columns\nuserPersonalityDF = followersVector.transform(userPersonalityDF)\nuserPersonalityDF = statusesVector.transform(userPersonalityDF)\n\n## Fit MinMaxScalerModel on our vectors and rescale\nfollowersScalerModel = followersScaler.fit(userPersonalityDF)\nstatusesScalerModel = statusesScaler.fit(userPersonalityDF)\n\nuserPersonalityDF = followersScalerModel.transform(userPersonalityDF)\nuserPersonalityDF = statusesScalerModel.transform(userPersonalityDF)\n\n## User-defined function to convert from vector back to float\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.sql.functions import lit, udf\n\ndef ith_(v, i):\n    try:\n        return float(v[i])\n    except ValueError:\n        return None\n\nudfVecToFloat = udf(ith_, DoubleType())\n\n## Add column for floating point scaled followers\nuserPersonalityDF = userPersonalityDF.withColumn(\"SCALED_FOLLOWERS\", udfVecToFloat('USER_FOLLOWERS_COUNT_SCALED', lit(0)))\\\n                                     .withColumn(\"SCALED_STATUSES\", udfVecToFloat('USER_STATUSES_COUNT_SCALED', lit(0)))\\\n                                     .select(\"USER_SCREEN_NAME\", \"SENTIMENT_LABEL\", \"SENTIMENT\", \"SCALED_FOLLOWERS\", \\\n                                            \"SCALED_STATUSES\", \"OPENNESS\", \"CONSCIENTIOUSNESS\", \"EXTRAVERSION\", \\\n                                            \"AGREEABLENESS\", \"NEUROTICISM\")\n        \n## Lastly, we'll have to add columns for features both with and without Personality Insights\nassemblerWithPI = VectorAssembler(\n    inputCols = ['SENTIMENT','SCALED_FOLLOWERS','SCALED_STATUSES','OPENNESS','CONSCIENTIOUSNESS','EXTRAVERSION',\\\n               'AGREEABLENESS','NEUROTICISM'],\n    outputCol = \"PI_ENRICHED_FEATURES\")\n\nassemblerWithoutPI = VectorAssembler(\n    inputCols = ['SENTIMENT', 'SCALED_FOLLOWERS', 'SCALED_STATUSES'],\n    outputCol = \"BASE_FEATURES\")\n\n\nuserPersonalityDF = assemblerWithPI.transform(userPersonalityDF)\nuserPersonalityDF = assemblerWithoutPI.transform(userPersonalityDF)\n\n## View transformed DF\nuserPersonalityDF.limit(5).toPandas()"
        }, 
        {
            "source": "### Kmeans clustering \n\nNow that we have prepared the data, we can run Kmeans to assign cluster labels to each user.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.clustering import KMeans\n\n## Define model parameters and set the seed\nbaseKMeans = KMeans(featuresCol = \"BASE_FEATURES\", predictionCol = \"BASE_PREDICTIONS\").setK(5).setSeed(206)\npiKMeans = KMeans(featuresCol = \"PI_ENRICHED_FEATURES\", predictionCol = \"PI_PREDICTIONS\").setK(5).setSeed(206)\n\n## Fit model on our feature vectors\nbaseClustersFit = baseKMeans.fit(userPersonalityDF.select(\"BASE_FEATURES\"))\nenrichedClustersFit = piKMeans.fit(userPersonalityDF.select(\"PI_ENRICHED_FEATURES\"))\n\n## Get the cluster IDs for each user\nuserPersonalityDF = baseClustersFit.transform(userPersonalityDF)\nuserPersonalityDF = enrichedClustersFit.transform(userPersonalityDF)\n\n## Check our work\nuserPersonalityDF.limit(5).toPandas()"
        }, 
        {
            "source": "____________\n\n<a id=\"clusters\"> </a>\n## Step 10: Visualize user clusters\n\nOK, great - we have successfully clustered our users both with and without enrichment data from Watson Personality Insights. Now we can visualize users through their cluster identity.\n \nIn this step, we visualize the different user clusters obtained with and without using Personality traits. This visualization illustrates the differences between two clustering approachs and shows how Watson <a href=\"https://www.ibm.com/watson/developercloud/personality-insights.html\" target=\"_blank\" rel=\"noopener no referrer\">Personality Insights</a> can help provide finer user segmentation.\n\nBrand managers and marketing teams can use this segmentation to craft different messaging to target the various user segments to improve brand adoption in the marketplace.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "## Aggregate users in each cluster and convert to Pandas for plotting\nbaseClusterAggDF = userPersonalityDF.groupBy('BASE_PREDICTIONS').agg(F.count('USER_SCREEN_NAME').alias('NUM_USERS')).toPandas()\nenrichedClusterAggDF = userPersonalityDF.groupBy('PI_PREDICTIONS').agg(F.count('USER_SCREEN_NAME').alias('NUM_USERS')).toPandas()\n\n%matplotlib inline\n# Code courtesy of \n#pandas_softdrink_tweets_grouped_by_sentiment=df_softdrink_tweets_grouped_by_sentiment.toPandas()\n#pandas_softdrink_tweets_grouped_by_sentiment.count()\nplot1_labels = baseClusterAggDF['BASE_PREDICTIONS']\nplot1_values = baseClusterAggDF['NUM_USERS']\nplot1_colors = ['green', 'gray', 'red', 'blue', 'yellow']\nplot2_labels = enrichedClusterAggDF['PI_PREDICTIONS']\nplot2_values = enrichedClusterAggDF['NUM_USERS']\nplot2_colors = ['green', 'gray', 'red', 'blue', 'yellow']\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(23, 10))\naxes[0].pie(plot1_values,  labels=plot1_labels, colors=plot1_colors, autopct='%1.1f%%')\naxes[0].set_title('Cluster distrubtion without Personality traits')\naxes[0].set_aspect('equal')\naxes[0].legend(loc=\"upper right\", labels=plot1_labels)\naxes[1].pie(plot2_values,  labels=plot2_labels, colors=plot2_colors, autopct='%1.1f%%')\naxes[1].set_title('Cluster distrubtion with Personality traits')\naxes[1].set_aspect('equal')\naxes[1].legend(loc=\"upper right\", labels=plot2_labels)\nfig.subplots_adjust(hspace=1)\nplt.show()"
        }, 
        {
            "source": "### Scatter plots of clusters using Principal Components Analysis (PCA)\n\nTypically you would visualize clusters by plotting some aggregate measure of the data, then filling in the data points with different colors based on cluster ID. In the absence of aggregate metrics, we can use Principal Components Analysis (PCA) to compress our data set down to two dimensions. After we've performed PCA we can then plot the values of the two components on the X and Y axis to form a scatterplot.  \n\nLet's try that now.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.feature import PCA\n\n## Get the first two principal components for features with and without enrichment from Personality Insights\npcaBase = PCA(k = 2, inputCol = \"BASE_FEATURES\", outputCol = \"pcaFeaturesBase\")\npcaEnriched = PCA(k = 2, inputCol = \"PI_ENRICHED_FEATURES\", outputCol = \"pcaFeaturesEnriched\")\n\n## Fit the model to our data\npcaBaseModel = pcaBase.fit(userPersonalityDF)\npcaEnrichedModel = pcaEnriched.fit(userPersonalityDF)\n\n## Transform the data get our principal components\nuserPersonalityDF = pcaBaseModel.transform(userPersonalityDF)\nuserPersonalityDF = pcaEnrichedModel.transform(userPersonalityDF)"
        }, 
        {
            "source": "We have the principal components, but before we can plot them we have to convert them from a feature vector back to individual columns.  We'll accomplish this using a lambda function to build a RowRDD, then convert back to a DataFrame.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql import Row\n\n## Split the features vector into columns with the rdd-based API, then convert to DF and reorder columns\npcaDF = userPersonalityDF.select(\"pcaFeaturesBase\", \"pcaFeaturesEnriched\", \"USER_SCREEN_NAME\", \"BASE_PREDICTIONS\", \"PI_PREDICTIONS\")\\\n                         .rdd.map(lambda x: Row(**{'PC1_BASE': float(x[0][0]), \n                                                   'PC2_BASE': float(x[0][1]),\n                                                   'PC1_ENRICHED': float(x[1][0]),\n                                                   'PC2_ENRICHED': float(x[1][1]),\n                                                   'USER_SCREEN_NAME': str(x[2]),\n                                                   'BASE_PREDICTIONS': int(x[3]),\n                                                   'PI_PREDICTIONS': int(x[4])}))\\\n                         .toDF()\\\n                         .select(\"USER_SCREEN_NAME\", \"PC1_BASE\", \"PC2_BASE\", \"BASE_PREDICTIONS\",\\\n                                 \"PC1_ENRICHED\", \"PC2_ENRICHED\", \"PI_PREDICTIONS\")\npcaDF.limit(5).toPandas()"
        }, 
        {
            "source": "Now that the dimensionality of our dataset has been reduced to 2, we can view the components on a typical scatter plot.  Let's see how the clusters look from this perspective.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import plotly \nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\n\npcaDF = pcaDF.toPandas()\n\n## For Base Features\ndata = [go.Scatter(x = pcaDF.PC2_BASE, \n                   y = pcaDF.PC1_BASE,\n                   mode = 'markers',\n                   name = 'BASE_PREDICTIONS',\n                   marker = dict(color = pcaDF.BASE_PREDICTIONS, size = 12),\n                   text = pcaDF.BASE_PREDICTIONS\n                  )\n       ]\nplotly.offline.iplot(data)"
        }, 
        {
            "source": "What is interesting to note about the clustering without Personality Insights enrichment is that it appears to have a very clear stratification from -1 to 1.  These clusters appear to be somewhat intuitively grouped together.  What about when we run the same algorithm on the PI-enriched data?", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## For Enriched Features\ndata = [go.Scatter(x = pcaDF.PC2_ENRICHED, \n                   y = pcaDF.PC1_ENRICHED, \n                   mode = 'markers',\n                   name = 'Clusters with PI',\n                   marker = dict(color = pcaDF.PI_PREDICTIONS, size = 12),\n                   text = pcaDF.PI_PREDICTIONS\n                                 )\n       ]\n\nplotly.offline.iplot(data)"
        }, 
        {
            "source": "Not quite the same intuitive results.  Perhaps we should iterate over this and try a different clustering algorithm some time.\n\nNow that we've enriched the user data with Watson APIs and grouped them into clusters, we can track these clusters over time to see how they respond to various metrics.  This could be purchase history, click patterns, or the response to different advertisement campaigns.  As we build up this data set we'll be able to classify new users and what group they fall into, resulting in a stronger user segmentation model over time.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "_______\n\n## Summary\nWe've accomplished quite a lot in this notebook, so let's take a moment to review.\n\nFirst we loaded our tweet data from Db2 Warehouse on Cloud.  Then we enriched it with several Watson APIs - Natural Language Understanding and Personality Insights.  Using Spark and its elegant API we shaped our data, explored it visually, and prepared it for machine learning.  Our approach was to discover user segmentation by assigning cluster IDs to each data point.  Finally, we plotted the results of our clustering with the aid of principal components analysis.  \n\nWhile more work remains to be done - no one said data science was easy - we have seen how we can seamlessly move between Watson APIs, Spark, and Python using the Data Science Experience. \n_______", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Authors\n**Rafi Kurlansik** is an Open Source Solutions Engineer specializing in big data technologies, such as Hadoop and Spark. He's responsible for developing and delivering demonstrations of IBM tech to both enterprise clients and the larger analytics community. Kurlansik has hands-on experience with machine learning, natural language processing, data visualization, and dashboard development. If you're wondering where he comes down on the biggest data science debate of our day, Rafi is, in his own words, \"an avid R fan, especially RStudio!\"\n\n**Joseph Kozhaya** is an IBM Master Inventor and a Watson Solution Architect working closely with partners, clients, and universities to build cloud and cognitive solutions.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "Copyright \u00a9 IBM Corp. 2017. This notebook and its source code are released under the terms of the MIT License.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<div><br><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/51/IBM_logo.svg/640px-IBM_logo.svg.png\" width = 200 height = 200>\n</div><br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark", 
            "name": "python3", 
            "language": "python3"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}